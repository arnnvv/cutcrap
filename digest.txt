Directory structure:
└── cutcrap/
    ├── Dockerfile
    ├── go.mod
    ├── go.sum
    ├── main.go
    ├── .dockerignore
    ├── .env.example
    ├── pkg/
    │   ├── api/
    │   │   └── openrouter.go
    │   ├── chunker/
    │   │   └── chunker.go
    │   ├── config/
    │   │   └── config.go
    │   ├── transcript/
    │   │   ├── formatter.go
    │   │   └── speakers.go
    │   ├── utils/
    │   │   ├── errors.go
    │   │   └── files.go
    │   └── workers/
    │       └── pool.go
    └── .github/
        └── workflows/
            └── deploy.yml

================================================
File: Dockerfile
================================================
FROM golang:1.24.1-alpine3.21 as builder

WORKDIR /app
COPY . .
RUN --mount=type=cache,target=/root/.cache/go-build \
    CGO_ENABLED=0 GOOS=linux \
    go build \
    -trimpath \
    -ldflags="-s -w" \
    -o api

FROM gcr.io/distroless/static:nonroot
COPY --from=builder --chmod=0755 /app/api /api
USER nonroot:nonroot
ENTRYPOINT ["/api"]



================================================
File: go.mod
================================================
module github.com/arnnvv/cutcrap

go 1.24.1

require github.com/joho/godotenv v1.5.1



================================================
File: go.sum
================================================
github.com/joho/godotenv v1.5.1 h1:7eLL/+HRGLY0ldzfGMeQkb7vMd0as4CfYvUVzLqw0N0=
github.com/joho/godotenv v1.5.1/go.mod h1:f4LDr5Voq0i2e/R5DDNOoa2zzDfwtkZa6DnEwAbqwq4=



================================================
File: main.go
================================================
package main

import (
	"bytes"
	"context"
	"io"
	"log"
	"mime/multipart"
	"net/http"
	"strconv"
	"strings"
	"time"

	"github.com/arnnvv/cutcrap/pkg/chunker"
	"github.com/arnnvv/cutcrap/pkg/config"
	"github.com/arnnvv/cutcrap/pkg/workers"

	"github.com/joho/godotenv" // Keep if using .env loading
)

func enableCors(w *http.ResponseWriter) {
	(*w).Header().Set("Access-Control-Allow-Origin", "*")
	(*w).Header().Set("Access-Control-Allow-Methods", "POST, GET, OPTIONS")
	// Make sure Content-Type is allowed if you have specific checks
	(*w).Header().Set("Access-Control-Allow-Headers", "Content-Type, Authorization, X-Requested-With") // Example broader set
}

func main() {
	// Load .env file if you added godotenv
	err := godotenv.Load()
	if err != nil {
		log.Println("No .env file found or error loading .env file, using system env vars")
	}

	log.Println("Starting PDF processor service")
	cfg := config.Load()
	log.Printf("Configuration loaded: Port=%s, MaxConcurrent=%d, ChunkSize=%d, PdfApi=%s", cfg.Port, cfg.MaxConcurrent, cfg.ChunkSize, cfg.Pdf_api)

	http.HandleFunc("/process", func(w http.ResponseWriter, r *http.Request) {
		enableCors(&w)
		if r.Method == "OPTIONS" {
			w.WriteHeader(http.StatusOK)
			return
		}
		// Use the modified uploadHandler
		uploadHandler(cfg)(w, r)
	})

	log.Printf("Server starting on :%s", cfg.Port)
	log.Fatal(http.ListenAndServe(":"+cfg.Port, nil))
}

func uploadHandler(cfg *config.Config) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		startTime := time.Now()
		log.Printf("\n\n=== NEW REQUEST ===")
		log.Printf("From: %s | Method: %s | Content-Type: %s", r.RemoteAddr, r.Method, r.Header.Get("Content-Type")) // Log Content-Type
		defer func() {
			log.Printf("=== REQUEST COMPLETED IN %v ===\n", time.Since(startTime))
		}()

		// *** CHANGE HERE: Use ParseMultipartForm instead of ParseForm ***
		// Set a max memory limit (e.g., 32 MB) for parts stored in memory.
		// Larger parts will be stored on disk. Adjust as needed.
		const maxMemory = 32 << 20 // 32 MB
		if err := r.ParseMultipartForm(maxMemory); err != nil {
			log.Printf("MULTIPART FORM PARSE ERROR: %v", err)
			// Check if it's because the Content-Type wasn't multipart
			if !strings.HasPrefix(r.Header.Get("Content-Type"), "multipart/form-data") {
				http.Error(w, "Invalid request format: Expected multipart/form-data", http.StatusBadRequest)
			} else {
				http.Error(w, "Invalid form data", http.StatusBadRequest)
			}
			return
		}
		// ***************************************************************

		// You can still use r.FormValue after ParseMultipartForm is called successfully
		text := r.FormValue("text")
		ratioStr := r.FormValue("ratio")
		mode := r.FormValue("mode")

		log.Printf("Received Form Data: text(len)=%d, ratio='%s', mode='%s'", len(text), ratioStr, mode) // Add log to see received values

		if text == "" {
			log.Printf("VALIDATION FAILED: Empty text field received") // Updated log message
			http.Error(w, "Text field is missing or empty", http.StatusBadRequest)
			return
		}

		ratio, err := strconv.ParseFloat(ratioStr, 64)
		// Ratio check should be <= 0 (or perhaps a small positive epsilon like 0.01)
		if err != nil || ratio <= 0 || ratio > 1 {
			log.Printf("VALIDATION FAILED: Invalid ratio '%v'", ratioStr)
			http.Error(w, "Invalid ratio value (must be > 0 and <= 1)", http.StatusBadRequest)
			return
		}

		if mode == "" {
			log.Printf("VALIDATION FAILED: Mode field is missing, defaulting to 'document'")
			mode = "document" // Defaulting might be okay, but log it
		}

		if mode != "document" && mode != "transcript" {
			log.Printf("VALIDATION FAILED: Invalid mode value '%s'", mode)
			http.Error(w, "Invalid mode value (must be 'document' or 'transcript')", http.StatusBadRequest)
			return
		}

		ctx, cancel := context.WithTimeout(r.Context(), 5*time.Minute)
		defer cancel()

		inputWordCount := len(strings.Fields(text))
		log.Printf("PROCESSING START | Mode: %s | Words: %d | Ratio: %.2f", mode, inputWordCount, ratio)

		var results []string
		var combinedResult string // Declare outside the if/else

		if mode == "transcript" {
			result := workers.ProcessTranscript(ctx, text, cfg, ratio)
			// Check for actual processing error if result is empty
			if result == "" {
				// It's possible the processing itself returned empty, not necessarily an internal server error.
				// Check context cancellation or specific errors if available.
				if ctx.Err() != nil {
					log.Printf("Transcript processing failed due to context error: %v", ctx.Err())
					http.Error(w, "Transcript processing timed out or was cancelled", http.StatusRequestTimeout)

				} else {
					log.Printf("Transcript processing returned an empty result.")
					// Decide if empty result is an error or valid outcome. Assuming valid for now.
					combinedResult = "" // Explicitly set to empty
				}

			} else {
				results = []string{result}
				combinedResult = combineResults(results)
			}
		} else { // document mode
			chunks, err := chunker.ChunkText(text, cfg.ChunkSize)
			if err != nil {
				log.Printf("Text chunking failed: %v", err)
				http.Error(w, "Text chunking failed", http.StatusInternalServerError)
				return
			}
			// Inside the uploadHandler function, within the 'else' block for document mode:
			results = workers.ProcessChunks(ctx, chunks, cfg, ratio, "document", "") // Add empty string for speakerAnalysis
			// Check context error after processing chunks
			if ctx.Err() != nil {
				log.Printf("Chunk processing failed due to context error: %v", ctx.Err())
				http.Error(w, "Document processing timed out or was cancelled", http.StatusRequestTimeout)
				return
			}
			combinedResult = combineResults(results)
		}

		// Only proceed to response if context is still valid
		if ctx.Err() == nil {
			outputWordCount := len(strings.Fields(combinedResult))
			reduction := 0.0
			if inputWordCount > 0 {
				reduction = 100.0 - (float64(outputWordCount)/float64(inputWordCount))*100.0
			}
			log.Printf("RESPONSE READY | Input: %d words | Output: %d words | Reduction: %.1f%%",
				inputWordCount, outputWordCount, reduction)

			// Check if PDF generation is requested and possible
			// The check for "# " might be too simplistic. Maybe rely on mode?
			// Or add a specific request parameter/header if PDF is desired?
			// Let's assume for now: PDF only if document mode AND headings found AND PDF_API is set.
			shouldGeneratePdf := mode == "document" && cfg.Pdf_api != "" && strings.Contains(combinedResult, "# ")

			if shouldGeneratePdf {
				log.Printf("Attempting PDF generation via API: %s", cfg.Pdf_api)
				w.Header().Set("Content-Type", "application/pdf")
				// Use a more descriptive filename
				w.Header().Set("Content-Disposition", "attachment; filename=processed_document.pdf")

				var body bytes.Buffer
				mpWriter := multipart.NewWriter(&body)
				// Use a more appropriate filename for the markdown content
				fileWriter, err := mpWriter.CreateFormFile("file", "content.md")
				if err != nil {
					log.Printf("PDF API FORM CREATION FAILED: %v", err)
					http.Error(w, "PDF generation setup failed", http.StatusInternalServerError)
					return
				}

				if _, err := fileWriter.Write([]byte(combinedResult)); err != nil {
					log.Printf("PDF API WRITE FAILED: %v", err)
					http.Error(w, "PDF generation content write failed", http.StatusInternalServerError)
					return
				}
				mpWriter.Close() // Must close before reading body or setting content type

				req, err := http.NewRequestWithContext(ctx, "POST", cfg.Pdf_api, &body)
				if err != nil {
					log.Printf("PDF API REQUEST CREATION FAILED: %v", err)
					http.Error(w, "PDF generation request creation failed", http.StatusInternalServerError)
					return
				}
				// Set the correct multipart content type for the PDF API request
				req.Header.Set("Content-Type", mpWriter.FormDataContentType())

				// Use a client with a timeout relevant to PDF generation
				client := &http.Client{Timeout: 2 * time.Minute}
				resp, err := client.Do(req)
				if err != nil {
					log.Printf("PDF API REQUEST FAILED: %v", err)
					http.Error(w, "PDF generation request failed", http.StatusInternalServerError)
					return
				}
				defer resp.Body.Close()

				if resp.StatusCode != http.StatusOK {
					respBodyBytes, _ := io.ReadAll(resp.Body) // Try to read error body
					log.Printf("PDF API RETURNED STATUS: %d. Body: %s", resp.StatusCode, string(respBodyBytes))
					http.Error(w, "PDF generation failed on external API", http.StatusInternalServerError)
					return
				}

				// Stream the PDF response back to the original client
				if _, err := io.Copy(w, resp.Body); err != nil {
					log.Printf("PDF STREAM FAILED: %v", err)
					// Don't send another http.Error if header is already sent
					// http.Error(w, "Failed to stream PDF response", http.StatusInternalServerError)
					return
				}
			} else {
				// Default to plain text if not generating PDF
				log.Printf("Sending response as plain text.")
				w.Header().Set("Content-Type", "text/plain; charset=utf-8")
				// Set appropriate filename for text download
				if mode == "transcript" {
					w.Header().Set("Content-Disposition", "attachment; filename=processed_transcript.txt")
				} else {
					w.Header().Set("Content-Disposition", "attachment; filename=processed_document.txt")
				}
				io.WriteString(w, combinedResult)
			}
		} // End check for ctx.Err() == nil

	}
}

// combineResults remains the same
func combineResults(results []string) string {
	// Filter out empty strings that might result from failed chunk processing
	var validResults []string
	for _, res := range results {
		if res != "" {
			validResults = append(validResults, res)
		}
	}

	var final strings.Builder
	for i, res := range validResults {
		final.WriteString(res)
		// Add separator only between valid chunks
		if i < len(validResults)-1 {
			// Use a single newline for transcript mode for tighter formatting?
			// Or keep double newline for both for consistency? Let's keep double for now.
			final.WriteString("\n\n")
		}
	}
	return final.String()
}



================================================
File: .dockerignore
================================================
.env
.git/
.github/
.gitignore
.env.example



================================================
File: .env.example
================================================
PORT=
OPENROUTER_API_KEY=
MAX_CONCURRENT=
REQUEST_TIMEOUT=
CHUNK_SIZE=
TARGET_WORD_COUNT=
CHUNK_OVERLAP=
PDF_API=



================================================
File: pkg/api/openrouter.go
================================================
package api

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"strings"
	"time"
)

// GeminiResponse struct remains the same
type GeminiResponse struct {
	Candidates []struct {
		Content struct {
			Parts []struct {
				Text string `json:"text"`
			} `json:"parts"`
			Role string `json:"role"`
		} `json:"content"`
		FinishReason string  `json:"finishReason"`
		AvgLogprobs  float64 `json:"avgLogprobs"`
	} `json:"candidates"`
	UsageMetadata struct {
		PromptTokenCount     int `json:"promptTokenCount"`
		CandidatesTokenCount int `json:"candidatesTokenCount"`
		TotalTokenCount      int `json:"totalTokenCount"`
		// ... rest of UsageMetadata fields
	} `json:"usageMetadata"`
	ModelVersion string `json:"modelVersion"`
}

// --- NEW FUNCTION for Speaker Analysis ---
func AnalyzeSpeakers(ctx context.Context, fullText, apiKey string) (string, error) {
	startTime := time.Now()
	log.Printf("Starting speaker analysis for text of %d words", len(strings.Fields(fullText)))

	// Carefully craft the prompt for analysis
	analysisPrompt := `Analyze the following podcast transcript to identify the speakers. Provide the following information in a clear, concise list format:
1. Total number of distinct speakers detected.
2. Identify the HOST (usually the one asking questions, leading the conversation, or doing intros/outros). Provide their name if clearly mentioned.
3. Identify the GUEST(s). Provide their names if clearly mentioned. If multiple guests, list them as Guest 1, Guest 2, etc.
4. For each speaker (Host and Guests), provide a brief 1-sentence description of their apparent role or topic focus if discernible from the text.

Focus ONLY on information present in the transcript. Do not guess information not present.

Transcript:
--- TRANSCRIPT START ---
%s
--- TRANSCRIPT END ---

Return ONLY the analysis result using this exact format:
- Total Speakers: [Number]
- Host: [Name or "Host"], [Brief Description]
- Guest 1: [Name or "Guest 1"], [Brief Description]
- Guest 2: [Name or "Guest 2"], [Brief Description]
... (continue for all detected guests)`

	payload := map[string]any{
		"contents": []map[string]any{
			{
				"parts": []map[string]string{
					{
						"text": fmt.Sprintf(analysisPrompt, fullText),
					},
				},
			},
		},
		// Optional: Add generationConfig if needed (e.g., temperature for analysis)
		// "generationConfig": map[string]any{
		// 	"temperature": 0.3, // Lower temperature for more factual analysis
		// },
	}

	body, err := json.Marshal(payload)
	if err != nil {
		log.Printf("Speaker analysis: Failed to marshal payload: %v", err)
		return "", fmt.Errorf("failed to create analysis request body: %w", err)
	}

	// Use the same Gemini endpoint
	// Consider if a different model might be better for analysis, but flash should be okay.
	apiURL := "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?key=" + apiKey
	log.Printf("Preparing Speaker Analysis API request to Gemini API")
	req, err := http.NewRequestWithContext(ctx, "POST", apiURL, bytes.NewReader(body))
	if err != nil {
		log.Printf("Speaker analysis: Failed to create API request: %v", err)
		return "", fmt.Errorf("failed to create analysis request: %w", err)
	}

	req.Header.Set("Content-Type", "application/json")

	// Use a longer timeout for potentially large analysis tasks? Or keep standard?
	client := &http.Client{Timeout: 90 * time.Second} // Increased timeout for analysis
	log.Printf("Sending speaker analysis request to Gemini API")
	resp, err := client.Do(req)
	if err != nil {
		log.Printf("Speaker analysis API request failed: %v", err)
		return "", fmt.Errorf("analysis API request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		respBodyBytes, _ := io.ReadAll(resp.Body)
		log.Printf("Speaker analysis API request returned non-OK status: %s. Body: %s", resp.Status, string(respBodyBytes))
		return "", fmt.Errorf("analysis API request failed: %s", resp.Status)
	}
	log.Printf("Received speaker analysis response from Gemini API with status %s", resp.Status)

	var response GeminiResponse
	if err := json.NewDecoder(resp.Body).Decode(&response); err != nil {
		log.Printf("Speaker analysis: Failed to decode API response: %v", err)
		return "", fmt.Errorf("failed to decode analysis response: %w", err)
	}

	if len(response.Candidates) == 0 || len(response.Candidates[0].Content.Parts) == 0 {
		log.Printf("Speaker analysis API response contained no content")
		return "", fmt.Errorf("no content in analysis response")
	}

	analysisResult := response.Candidates[0].Content.Parts[0].Text
	log.Printf("Successfully completed speaker analysis in %v. Result:\n%s", time.Since(startTime), analysisResult)
	return analysisResult, nil
}

// --- END NEW FUNCTION ---

// Modify ProcessText to handle both modes, accepting speakerAnalysis for transcript mode
// No need for separate ProcessTranscript function here anymore.
func ProcessTextWithMode(ctx context.Context, text, apiKey string, targetWordCount int, mode string, speakerAnalysis string) (string, error) {
	startTime := time.Now()
	inputWordCount := len(strings.Fields(text))
	log.Printf("Processing text chunk of %d words (target: %d words) in %s mode",
		inputWordCount, targetWordCount, mode)

	var prompt string
	if mode == "transcript" {
		// --- DYNAMIC TRANSCRIPT PROMPT ---
		if speakerAnalysis == "" {
			log.Printf("Warning: Processing transcript chunk without speaker analysis context.")
			// Fallback to a generic prompt if analysis is missing
			speakerAnalysis = "- Total Speakers: Unknown\n- Host: Host\n- Guest 1: Guest"
		}

		prompt = fmt.Sprintf(`You are processing a chunk of subtitles from a YouTube podcast. Use the following speaker analysis to format the text as a clean, readable transcript:

--- Speaker Analysis START ---
%s
--- Speaker Analysis END ---

Apply these rules to the CURRENT CHUNK below:
1. Identify speakers within this chunk based on the Speaker Analysis provided above.
2. Use the speaker identifiers (e.g., "Host, John Doe:", "Guest 1, Jane Smith:", "Host:", "Guest 1:") exactly as listed in the analysis.
3. Use extremely simple English suitable for a 10-year-old child, with basic vocabulary only.
4. Keep the spoken words almost identical to the original subtitles, but improve grammar, spelling, and sentence structure slightly for readability.
5. Do not use advanced vocabulary, idioms, or complicated expressions.
6. Format the output strictly line-by-line like this, starting each line with the correct speaker identifier from the analysis:
[Speaker Identifier]: [Exact simplified speech for that line/turn]
[Another Speaker Identifier]: [Exact simplified speech for that line/turn]

Important Constraints:
- Return ONLY the formatted transcript lines for THIS CHUNK.
- Do NOT add any introductory text, concluding remarks, summaries, explanations, or comments.
- Do NOT repeat the Speaker Analysis in your response.
- Ensure every line of dialogue starts with a speaker identifier from the analysis.

--- CURRENT CHUNK START ---
%s
--- CURRENT CHUNK END ---

Formatted Output:`, speakerAnalysis, text)
		// --- END DYNAMIC PROMPT ---

	} else { // document mode
		prompt = fmt.Sprintf(`Condense this text to approximately %d words while:
- Preserving all key plot points and essential information and data.
- Removing redundant descriptions and unnecessary elaborations.
- Using extremely simple English with basic vocabulary (like for a 10-year-old).
- Maintaining the original narration style as much as possible.
- If you identify any headings in the text, format them as "# Heading" on their own line in markdown style.

Important: Return ONLY the condensed text without any introductions, explanations, or summaries. Do not include phrases like "Here's the condensed version" or "In summary". Just provide the rewritten text directly.

--- TEXT TO CONDENSE START ---
%s
--- TEXT TO CONDENSE END ---

Condensed Text:`, targetWordCount, text) // Target word count is less strict now, "approximately"
	}

	payload := map[string]any{
		"contents": []map[string]any{
			{
				"parts": []map[string]string{
					{
						"text": prompt, // Use the generated prompt
					},
				},
			},
		},
		// Optional: Add generationConfig if needed
		"generationConfig": map[string]any{
			// Adjust temperature based on mode? Lower for transcript, maybe higher for creative condensing?
			"temperature": 0.4,
			// Consider maxOutputTokens if chunks can be very large
		},
	}

	body, _ := json.Marshal(payload)
	// Use a model appropriate for the task. 1.5 Flash is good for speed/cost.
	apiURL := "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=" + apiKey
	log.Printf("Preparing API request to Gemini API (%s mode)", mode)
	req, _ := http.NewRequestWithContext(ctx, "POST", apiURL, bytes.NewReader(body))

	req.Header.Set("Content-Type", "application/json")

	// Keep standard timeout for chunk processing
	client := &http.Client{Timeout: 60 * time.Second} // Slightly longer timeout for safety
	log.Printf("Sending request to Gemini API (%s mode)", mode)
	resp, err := client.Do(req)
	if err != nil {
		log.Printf("API request failed (%s mode): %v", mode, err)
		return "", err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		respBodyBytes, _ := io.ReadAll(resp.Body)
		log.Printf("API request returned non-OK status (%s mode): %s. Body: %s", mode, resp.Status, string(respBodyBytes))
		return "", fmt.Errorf("API request failed (%s mode): %s", mode, resp.Status)
	}
	log.Printf("Received response from Gemini API with status %s (%s mode)", resp.Status, mode)

	var response GeminiResponse
	if err := json.NewDecoder(resp.Body).Decode(&response); err != nil {
		log.Printf("Failed to decode API response (%s mode): %v", mode, err)
		return "", err
	}

	if len(response.Candidates) == 0 || len(response.Candidates[0].Content.Parts) == 0 {
		log.Printf("API response contained no content (%s mode)", mode)
		return "", fmt.Errorf("no content in response (%s mode)", mode)
	}

	result := response.Candidates[0].Content.Parts[0].Text
	outputWordCount := len(strings.Fields(result))
	reductionPercent := 100.0
	if inputWordCount > 0 {
		// Reduction calculation might not make sense for transcript formatting, only for condensing.
		if mode == "document" {
			reductionPercent = 100.0 - (float64(outputWordCount)/float64(inputWordCount))*100.0
			log.Printf("Successfully processed text in %v, result length: %d words (reduced from %d words, %.1f%% reduction)",
				time.Since(startTime), outputWordCount, inputWordCount, reductionPercent)
		} else {
			log.Printf("Successfully processed transcript chunk in %v, result length: %d words (input: %d words)",
				time.Since(startTime), outputWordCount, inputWordCount)
		}
	}

	return result, nil
}

// Remove the old separate ProcessText and ProcessTranscript functions
// func ProcessText(...) { ... }
// func ProcessTranscript(...) { ... }



================================================
File: pkg/chunker/chunker.go
================================================
package chunker

import (
	"log"
	"strings"
	"unicode"
)

func ChunkText(content string, chunkSize int) ([]string, error) {
	log.Printf("Starting text chunking with chunk size %d words", chunkSize)

	content = strings.ReplaceAll(content, "\r\n", " ")
	content = strings.ReplaceAll(content, "\n", " ")

	sentences := splitIntoSentences(content)
	log.Printf("Split content into %d sentences", len(sentences))

	return createChunksFromSentences(sentences, chunkSize), nil
}

func ChunkTextBySpace(content string, chunkSize int, overlap int) ([]string, error) {
	log.Printf("Starting space-based text chunking with chunk size %d words and %d words overlap",
		chunkSize, overlap)

	content = strings.ReplaceAll(content, "\r\n", " ")
	content = strings.ReplaceAll(content, "\n", " ")

	content = strings.Join(strings.Fields(content), " ")

	words := strings.Fields(content)
	log.Printf("Text contains %d words total", len(words))

	var chunks []string

	if len(words) <= chunkSize {
		log.Printf("Text is smaller than chunk size, returning as single chunk")
		return []string{content}, nil
	}

	for i := 0; i < len(words); i += (chunkSize - overlap) {
		end := i + chunkSize
		if end > len(words) {
                        end = len(words)
                }

		chunk := strings.Join(words[i:end], " ")
		chunks = append(chunks, chunk)

		if i > 0 && i%1000 == 0 {
			log.Printf("Created %d chunks so far", len(chunks))
		}

		if end == len(words) {
			break
		}
	}

	log.Printf("Created %d chunks using space-based chunking", len(chunks))
	return chunks, nil
}

func splitIntoSentences(text string) []string {
	log.Printf("Splitting text into sentences, text length: %d characters", len(text))

	text = replaceAbbreviations(text)

	var sentences []string
	var currentSentence strings.Builder

	for i := 0; i < len(text); i++ {
		currentSentence.WriteByte(text[i])

		if (text[i] == '.' || text[i] == '!' || text[i] == '?') &&
			(i == len(text)-1 || unicode.IsSpace(rune(text[i+1]))) {

			sentence := strings.TrimSpace(currentSentence.String())
			if len(strings.Fields(sentence)) > 0 {
				sentences = append(sentences, sentence)
			}
			currentSentence.Reset()
		}
	}

	if currentSentence.Len() > 0 {
		sentence := strings.TrimSpace(currentSentence.String())
		if len(strings.Fields(sentence)) > 0 {
			sentences = append(sentences, sentence)
		}
	}

	log.Printf("Found %d sentences in text", len(sentences))
	return sentences
}

func createChunksFromSentences(sentences []string, targetChunkSize int) []string {
	var chunks []string
	var currentChunk strings.Builder
	currentWordCount := 0

	for i, sentence := range sentences {
		sentenceWords := len(strings.Fields(sentence))

		if currentWordCount > 0 && currentWordCount+sentenceWords > targetChunkSize {
			chunk := strings.TrimSpace(currentChunk.String())
			chunks = append(chunks, chunk)
			log.Printf("Created chunk with %d words", currentWordCount)

			currentChunk.Reset()
			currentWordCount = 0
		}

		currentChunk.WriteString(sentence + " ")
		currentWordCount += sentenceWords

		if i > 0 && i%100 == 0 {
			log.Printf("Processed %d/%d sentences", i, len(sentences))
		}
	}

	if currentChunk.Len() > 0 {
		chunk := strings.TrimSpace(currentChunk.String())
		chunks = append(chunks, chunk)
		log.Printf("Created final chunk with %d words", currentWordCount)
	}

	log.Printf("Created %d chunks from %d sentences", len(chunks), len(sentences))
	return chunks
}

func replaceAbbreviations(text string) string {
	abbreviations := []string{
		"Mr.", "Mrs.", "Ms.", "Dr.", "Prof.",
		"Inc.", "Ltd.", "Co.", "Corp.",
		"i.e.", "e.g.", "etc.",
		"vs.", "a.m.", "p.m.",
		"U.S.", "U.K.", "E.U.",
	}

	result := text
	for _, abbr := range abbreviations {
		placeholder := strings.ReplaceAll(abbr, ".", "·")
		result = strings.ReplaceAll(result, abbr, placeholder)
	}

	return result
}



================================================
File: pkg/config/config.go
================================================
package config

import (
	"log"
	"os"
	"strconv"
	"time"
)

type Config struct {
	Port           string
	OpenRouterKey  string
	MaxConcurrent  int
	RequestTimeout time.Duration
	ChunkSize      int
	ChunkOverlap   int
	Pdf_api        string
}

func Load() *Config {
	log.Println("Loading configuration from environment")

	port := getEnv("PORT", "8080")
	log.Printf("PORT: %s", port)

	pdf_api := getEnv("PDF_API", "")
	apiKey := getEnv("OPENROUTER_API_KEY", "")
	if apiKey == "" {
		log.Printf("WARNING: OPENROUTER_API_KEY not set")
	} else {
		log.Printf("OPENROUTER_API_KEY: [REDACTED]")
	}

	maxConcurrent := getEnvAsInt("MAX_CONCURRENT", 10)
	log.Printf("MAX_CONCURRENT: %d", maxConcurrent)

	requestTimeout := getEnvAsDuration("REQUEST_TIMEOUT", 30*time.Second)
	log.Printf("REQUEST_TIMEOUT: %v", requestTimeout)

	chunkSize := getEnvAsInt("CHUNK_SIZE", 900)
	log.Printf("CHUNK_SIZE: %d", chunkSize)

	chunkOverlap := getEnvAsInt("CHUNK_OVERLAP", 100)
	log.Printf("CHUNK_OVERLAP: %d", chunkOverlap)

	return &Config{
		Port:           port,
		OpenRouterKey:  apiKey,
		MaxConcurrent:  maxConcurrent,
		RequestTimeout: requestTimeout,
		ChunkSize:      chunkSize,
		ChunkOverlap:   chunkOverlap,
		Pdf_api:        pdf_api,
	}
}

func getEnv(key, defaultValue string) string {
	value := os.Getenv(key)
	if value == "" {
		log.Printf("Environment variable %s not set, using default: %s", key, defaultValue)
		return defaultValue
	}
	return value
}

func getEnvAsInt(key string, defaultValue int) int {
	valueStr := getEnv(key, "")
	if valueStr == "" {
		return defaultValue
	}

	value, err := strconv.Atoi(valueStr)
	if err != nil {
		log.Printf("Failed to parse %s as integer: %v, using default: %d", key, err, defaultValue)
		return defaultValue
	}
	return value
}

func getEnvAsDuration(key string, defaultValue time.Duration) time.Duration {
	valueStr := getEnv(key, "")
	if valueStr == "" {
		return defaultValue
	}

	value, err := time.ParseDuration(valueStr)
	if err != nil {
		log.Printf("Failed to parse %s as duration: %v, using default: %v", key, err, defaultValue)
		return defaultValue
	}
	return value
}



================================================
File: pkg/transcript/formatter.go
================================================
package transcript

import (
	"fmt"
	"log"
	"regexp"
	"strings"
)

// --- REVISED FormatTranscript FUNCTION ---
// This function processes a single chunk received from the AI,
// aiming to group consecutive lines from the same speaker.
func FormatTranscript(text string) string {
	log.Println("Formatting transcript text chunk")

	// Basic cleanup first
	text = strings.ReplaceAll(text, "\r\n", "\n") // Normalize line endings
	text = strings.TrimSpace(text)                // Trim overall chunk

	if text == "" {
		return "" // Nothing to format
	}

	lines := strings.Split(text, "\n")
	var resultLines []string
	var currentSpeaker string = ""  // Track the speaker of the current block
	var currentText strings.Builder // Buffer for the current speaker's text

	speakerPattern := regexp.MustCompile(`^([^:]+):\s*(.*)$`) // Pattern to identify speaker lines

	flushSpeakerText := func() {
		if currentSpeaker != "" && currentText.Len() > 0 {
			// Add the accumulated text for the previous speaker
			resultLines = append(resultLines, fmt.Sprintf("%s: %s", currentSpeaker, strings.TrimSpace(currentText.String())))
			currentText.Reset() // Clear buffer for the next speaker block
			// currentSpeaker = "" // Reset speaker *after* flushing
		}
	}

	for _, line := range lines {
		trimmedLine := strings.TrimSpace(line)

		// Ignore empty lines within the chunk for grouping purposes
		// Blank lines for final output spacing are handled in CombineTranscriptChunks
		if trimmedLine == "" {
			continue
		}

		speakerMatch := speakerPattern.FindStringSubmatch(trimmedLine)

		if len(speakerMatch) == 3 { // It's a speaker line (e.g., "Host, Name: Speech")
			speaker := strings.TrimSpace(speakerMatch[1])
			speech := speakerMatch[2] // Keep potential leading/trailing space in speech for now

			if speaker == currentSpeaker {
				// Same speaker continues, append speech
				if currentText.Len() > 0 {
					currentText.WriteString(" ") // Add space between appended lines
				}
				currentText.WriteString(speech)
			} else {
				// New speaker starts
				flushSpeakerText()              // Write out the previous speaker's complete text block
				currentSpeaker = speaker        // Set the new current speaker
				currentText.WriteString(speech) // Start the new speaker's text buffer
			}
		} else {
			// Not a speaker line (e.g., speech continued on a new line without a tag)
			if currentSpeaker != "" {
				// Append this line to the current speaker's text buffer
				if currentText.Len() > 0 {
					currentText.WriteString(" ") // Add space
				}
				currentText.WriteString(trimmedLine) // Append the whole line
			} else {
				// Text before the first speaker tag - less likely with current prompt but handle defensively
				// Or orphaned text. Let's append it as its own line for now.
				log.Printf("Warning: Encountered non-speaker line before first speaker or orphaned: '%s'", trimmedLine)
				resultLines = append(resultLines, trimmedLine)
			}
		}
	}

	// Flush any remaining text for the last speaker after the loop finishes
	flushSpeakerText()

	// Join the grouped lines for this chunk
	processedChunk := strings.Join(resultLines, "\n") // Use single newline between speaker blocks within chunk

	log.Printf("Transcript chunk formatting completed. Output lines: %d", len(resultLines))
	return processedChunk
}

// --- CombineTranscriptChunks function (Mostly the same as the previous corrected version) ---
// It now receives chunks where same-speaker lines should already be grouped.
func CombineTranscriptChunks(chunks []string) string {
	log.Printf("Combining and applying final formatting to %d transcript chunks", len(chunks))

	var processedChunks []string
	for _, chunk := range chunks {
		// FormatTranscript above should handle grouping within the chunk
		formattedChunk := FormatTranscript(chunk)
		if formattedChunk != "" {
			processedChunks = append(processedChunks, formattedChunk)
		}
	}

	// Join the processed chunks with double newlines (to create one blank line between chunks/turns)
	combined := strings.Join(processedChunks, "\n\n")

	// --- Start Final Formatting (Name extraction and Bolding) ---

	combined = regexp.MustCompile(`\n{3,}`).ReplaceAllString(combined, "\n\n")
	combined = strings.TrimSpace(combined)

	var finalLines []string
	speakerLineRegex := regexp.MustCompile(`^([^:]+):\s*(.*)$`)
	lines := strings.Split(combined, "\n")

	for _, line := range lines {
		if line == "" { // Preserve intentional blank lines
			finalLines = append(finalLines, "")
			continue
		}

		trimmedLine := strings.TrimSpace(line)
		if trimmedLine == "" { // Skip lines that were only whitespace
			continue
		}

		if matches := speakerLineRegex.FindStringSubmatch(trimmedLine); len(matches) == 3 {
			speakerPart := strings.TrimSpace(matches[1])
			speechPart := matches[2]
			extractedName := speakerPart

			if commaIndex := strings.Index(speakerPart, ", "); commaIndex != -1 {
				extractedName = strings.TrimSpace(speakerPart[commaIndex+2:])
			}

			formattedLine := fmt.Sprintf("**%s**: %s", extractedName, speechPart)
			finalLines = append(finalLines, formattedLine)
		} else {
			// Handle lines that are not blank and not speaker lines (should be less common now)
			log.Printf("Warning: Skipping non-speaker line during final formatting: '%s'", trimmedLine)
			// Optionally keep them: finalLines = append(finalLines, trimmedLine)
		}
	}

	finalOutput := strings.Join(finalLines, "\n")
	finalOutput = regexp.MustCompile(`\n{3,}`).ReplaceAllString(finalOutput, "\n\n") // Final cleanup
	finalOutput = strings.TrimSpace(finalOutput)

	log.Printf("Successfully combined and formatted transcript. Final word count: %d", len(strings.Fields(finalOutput)))
	return finalOutput
}



================================================
File: pkg/transcript/speakers.go
================================================
package transcript

import (
	"log"
	"regexp"
	"strings"
)

type SpeakerInfo struct {
	OriginalLabel string
	StandardLabel string
	Occurrences   int
}

func DetectSpeakers(text string) map[string]string {
	log.Println("Detecting speakers in transcript text")

	patterns := []*regexp.Regexp{
		regexp.MustCompile(`(?m)^([A-Za-z][A-Za-z\s\.]{0,20}):\s`),
		regexp.MustCompile(`\[([A-Za-z][A-Za-z\s\.]{0,20})\]:`),
		regexp.MustCompile(`(?i)(speaker|person)\s*([a-z0-9])(\s|:)`),
		regexp.MustCompile(`(?i)(host|guest|interviewer|interviewee)(\s|:)`),
	}

	speakerMap := make(map[string]SpeakerInfo)

	for _, pattern := range patterns {
		matches := pattern.FindAllStringSubmatch(text, -1)
		for _, match := range matches {
			if len(match) > 1 {
				speaker := strings.TrimSpace(match[1])
				if speaker != "" {
					info, exists := speakerMap[speaker]
					if exists {
						info.Occurrences++
						speakerMap[speaker] = info
					} else {
						speakerMap[speaker] = SpeakerInfo{
							OriginalLabel: speaker,
							StandardLabel: "",
							Occurrences:   1,
						}
					}
				}
			}
		}
	}

	result := make(map[string]string)

	for name := range speakerMap {
		if strings.EqualFold(name, "Guest") || strings.EqualFold(name, "Host") {
			result[name+":"] = name + ":"
		}
	}

	log.Printf("Preserved %d speaker labels in transcript", len(result))
	return result
}

func StandardizeSpeakers(text string, speakerMap map[string]string) string {
	if len(speakerMap) == 0 {
		return text
	}

	log.Println("Standardizing speaker labels in transcript")

	result := text
	for original, standard := range speakerMap {
		pattern := regexp.MustCompile(`(?m)(^|\n)` + regexp.QuoteMeta(original) + `\s*`)
		result = pattern.ReplaceAllString(result, "${1}"+standard+" ")
	}

	return result
}



================================================
File: pkg/utils/errors.go
================================================
package utils

import "fmt"

type ProcessingError struct {
	Step    string
	Message string
	Err     error
}

func (e *ProcessingError) Error() string {
	return fmt.Sprintf("%s error: %s (%v)", e.Step, e.Message, e.Err)
}

func WrapError(step, message string, err error) error {
	return &ProcessingError{
		Step:    step,
		Message: message,
		Err:     err,
	}
}



================================================
File: pkg/utils/files.go
================================================
package utils

import (
	"fmt"
	"io"
	"mime/multipart"
	"os"
	"path/filepath"
	"strings"
)

func ValidatePDF(file multipart.File) error {
	buf := make([]byte, 4)
	_, err := file.Read(buf)
	if err != nil {
		return err
	}

	_, err = file.Seek(0, io.SeekStart)
	if err != nil {
		return err
	}

	if string(buf) != "%PDF" {
		return fmt.Errorf("not a valid PDF file")
	}

	return nil
}

func CreateTempFile(prefix string) (*os.File, string, error) {
	tmpFile, err := os.CreateTemp("", prefix+"_*.tmp")
	if err != nil {
		return nil, "", err
	}
	return tmpFile, tmpFile.Name(), nil
}

func SaveUploadedFile(file multipart.File, header *multipart.FileHeader) (string, error) {
	tmpFile, tmpPath, err := CreateTempFile("upload")
	if err != nil {
		return "", err
	}
	defer tmpFile.Close()

	_, err = io.Copy(tmpFile, file)
	if err != nil {
		os.Remove(tmpPath)
		return "", err
	}

	return tmpPath, nil
}

func CleanupTempFiles(paths ...string) {
	for _, path := range paths {
		os.Remove(path)
	}
}

func GetSafeFilename(original string) string {
	base := filepath.Base(original)
	ext := filepath.Ext(base)
	name := strings.TrimSuffix(base, ext)

	safe := strings.Map(func(r rune) rune {
		if (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || (r >= '0' && r <= '9') || r == '-' || r == '_' {
			return r
		}
		return '-'
	}, name)

	return safe + "-processed.txt"
}



================================================
File: pkg/workers/pool.go
================================================
package workers

import (
	"context"
	"fmt"
	"log"
	"strings"
	"sync"
	"time"

	"github.com/arnnvv/cutcrap/pkg/api"
	"github.com/arnnvv/cutcrap/pkg/chunker"
	"github.com/arnnvv/cutcrap/pkg/config"
	"github.com/arnnvv/cutcrap/pkg/transcript" // Keep for CombineTranscriptChunks
)

// Modify ProcessChunks to accept speakerAnalysis
func ProcessChunks(ctx context.Context, chunks []string, cfg *config.Config, ratio float64, mode string, speakerAnalysis string) []string {
	startTime := time.Now()

	totalInputWords := 0
	for _, chunk := range chunks {
		totalInputWords += len(strings.Fields(chunk))
	}

	log.Printf("Starting to process %d chunks with max concurrency %d (total input: %d words) in %s mode",
		len(chunks), cfg.MaxConcurrent, totalInputWords, mode)
	if mode == "transcript" {
		log.Printf("Using Speaker Analysis:\n%s", speakerAnalysis)
	}

	var (
		wg         sync.WaitGroup
		results    = make([]string, len(chunks)) // Pre-allocate results slice
		semaphore  = make(chan struct{}, cfg.MaxConcurrent)
		resultChan = make(chan struct { // Channel to receive results with index
			index   int
			content string
			err     error // Include error in channel communication
		})
	)

	// Launch workers in a separate goroutine
	go func() {
		defer close(resultChan) // Ensure channel is closed when all workers are done
		log.Printf("Worker dispatcher goroutine started, will process %d chunks", len(chunks))
		for i, chunk := range chunks {
			if ctx.Err() != nil { // Check for context cancellation before dispatching
				log.Printf("Context cancelled before dispatching chunk %d. Aborting further dispatch.", i)
				break // Stop dispatching new workers
			}
			wg.Add(1)
			semaphore <- struct{}{} // Acquire semaphore slot

			go func(index int, text string, analysis string) { // Pass analysis to worker
				chunkStartTime := time.Now()
				var processedContent string
				var processErr error
				logPrefix := fmt.Sprintf("Worker chunk %d", index)

				defer func() {
					log.Printf("%s completed in %v", logPrefix, time.Since(chunkStartTime))
					resultChan <- struct { // Send result (content or error) back
						index   int
						content string
						err     error
					}{index, processedContent, processErr}
					<-semaphore // Release semaphore slot
					wg.Done()
				}()

				// Check context within the worker too
				if ctx.Err() != nil {
					log.Printf("%s: Context cancelled before processing started.", logPrefix)
					processErr = ctx.Err()
					return
				}

				inputWords := len(strings.Fields(text))
				log.Printf("%s: Processing (%d words) in %s mode", logPrefix, inputWords, mode)

				// Calculate target word count based on ratio (less relevant for transcript formatting)
				// For transcript mode, ratio might determine desired density or summarization level *if* that was the goal.
				// For simple formatting, targetWordCount is less critical, but the API needs it.
				// Let's base it on the *original* chunk size for consistency.
				targetWordCount := int(float64(cfg.ChunkSize) * ratio) // Or maybe just a fixed reasonable number for transcripts?
				if targetWordCount <= 0 {
					targetWordCount = 1 // Ensure positive
				}
				// Log the target count being used
				log.Printf("%s: Using target word count for API: %d (based on ratio %.2f)", logPrefix, targetWordCount, ratio)

				// Use the consolidated API function
				processedContent, processErr = api.ProcessTextWithMode(ctx, text, cfg.OpenRouterKey, targetWordCount, mode, analysis)

				if processErr != nil {
					log.Printf("%s: Error during API processing: %v", logPrefix, processErr)
					// processedContent remains empty
				} else if ctx.Err() != nil {
					// Check context *after* API call returns, in case it finished just as context was cancelled
					log.Printf("%s: Context cancelled during or immediately after processing.", logPrefix)
					processErr = ctx.Err()
					processedContent = "" // Discard potentially partial result
				} else {
					outputWords := len(strings.Fields(processedContent))
					log.Printf("%s: Successfully processed, result: %d words", logPrefix, outputWords)
				}

			}(i, chunk, speakerAnalysis) // Pass index, chunk text, and speaker analysis
		}
		log.Println("Worker dispatcher: All workers dispatched, waiting for completion...")
		wg.Wait()
		log.Println("Worker dispatcher: All workers completed.")
	}() // End of worker dispatcher goroutine

	log.Println("Main thread: Collecting results from workers...")
	processedCount := 0
	errorCount := 0
	for res := range resultChan {
		processedCount++
		if res.err != nil {
			errorCount++
			// Keep results[res.index] as empty string (or nil if slice held pointers)
			log.Printf("Main thread: Received error for chunk %d: %v", res.index, res.err)
		} else {
			log.Printf("Main thread: Received result %d/%d for chunk %d (%d words)", processedCount, len(chunks), res.index, len(strings.Fields(res.content)))
			results[res.index] = res.content // Store successful result
		}
	}
	log.Printf("Main thread: Result collection complete. Processed: %d, Errors: %d", processedCount-errorCount, errorCount)

	// Post-processing (Combine results, calculate stats)
	validResults := 0
	totalOutputWords := 0
	var finalResults []string // Use a new slice for valid, ordered results

	for _, r := range results { // Iterate through the pre-allocated slice in order
		if r != "" { // Check if result is valid (not empty due to error or API issue)
			validResults++
			totalOutputWords += len(strings.Fields(r))
			finalResults = append(finalResults, r) // Add valid result to final list
		}
	}

	reductionPercent := 0.0 // Initialize
	if mode == "document" && totalInputWords > 0 {
		reductionPercent = 100.0 - (float64(totalOutputWords)/float64(totalInputWords))*100.0
		log.Printf("Processing completed in %v. Input: %d words, Output: %d words (%.1f%% reduction). Valid chunks: %d/%d",
			time.Since(startTime), totalInputWords, totalOutputWords, reductionPercent, validResults, len(chunks))
	} else {
		log.Printf("Transcript formatting completed in %v. Input: %d words, Output: %d words. Valid chunks: %d/%d",
			time.Since(startTime), totalInputWords, totalOutputWords, validResults, len(chunks))
	}

	// Combine transcript chunks *after* all processing is done
	if mode == "transcript" && validResults > 0 {
		log.Printf("Applying final transcript formatting/combining to %d valid chunks", validResults)
		// Use the 'finalResults' slice which contains only valid, ordered chunks
		combinedResult := transcript.CombineTranscriptChunks(finalResults)
		return []string{combinedResult} // Return the single combined string in a slice
	}

	// For document mode, return the slice of processed chunks (already filtered in finalResults)
	return finalResults
}

// Modify ProcessTranscript to perform analysis first
func ProcessTranscript(ctx context.Context, text string, cfg *config.Config, ratio float64) string {
	log.Printf("Processing transcript of %d words with ratio %.2f", len(strings.Fields(text)), ratio)

	// --- Step 1: Analyze Speakers ---
	speakerAnalysis, err := api.AnalyzeSpeakers(ctx, text, cfg.OpenRouterKey)
	if err != nil {
		// Decide how to handle analysis failure: proceed without context or fail?
		// Let's log the error and proceed with a warning (fallback handled in ProcessTextWithMode)
		log.Printf("WARNING: Speaker analysis failed: %v. Proceeding without speaker context.", err)
		speakerAnalysis = "" // Ensure it's empty if failed
	}
	if ctx.Err() != nil {
		log.Printf("Context cancelled during speaker analysis.")
		return "" // Or return context error?
	}
	// -----------------------------

	// --- Step 2: Chunk the Text ---
	// Chunking by space might be better for transcripts to avoid cutting mid-sentence/turn
	chunks, err := chunker.ChunkTextBySpace(text, cfg.ChunkSize, cfg.ChunkOverlap)
	if err != nil {
		log.Printf("Error chunking transcript text: %v", err)
		return ""
	}
	if len(chunks) == 0 {
		log.Printf("Transcript text resulted in zero chunks.")
		return ""
	}
	log.Printf("Chunked transcript into %d parts", len(chunks))
	// -----------------------------

	// --- Step 3: Process Chunks with Analysis Context ---
	// Pass speakerAnalysis to ProcessChunks
	results := ProcessChunks(ctx, chunks, cfg, ratio, "transcript", speakerAnalysis)
	// -----------------------------------------------------

	if ctx.Err() != nil {
		log.Printf("Context cancelled during chunk processing.")
		return ""
	}

	if len(results) == 0 {
		log.Printf("No valid results returned from transcript processing")
		return ""
	}

	// ProcessChunks now returns a slice containing the single combined result for transcripts
	return results[0]
}



================================================
File: .github/workflows/deploy.yml
================================================
name: Deploy

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Podman
        run: |
          sudo apt-get update
          sudo apt-get install -y podman

      - name: Log in to GHCR
        run: |
          echo "${{ secrets.GHCR_TOKEN }}" | podman login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Build and push image
        run: |
          IMAGE_TAG="ghcr.io/${{ github.repository }}:${{ github.sha }}"
          LATEST_TAG="ghcr.io/${{ github.repository }}:latest"

          echo "Building image: ${IMAGE_TAG}"
          podman build -t ${IMAGE_TAG} -t ${LATEST_TAG} .

          echo "Pushing image: ${IMAGE_TAG}"
          podman push ${IMAGE_TAG}

          echo "Pushing image: ${LATEST_TAG}"
          podman push ${LATEST_TAG}

      - name: Setup SSH key
        env:
          VM_SSH_KEY: ${{ secrets.VM_SSH_KEY }}
        run: |
          mkdir -p ~/.ssh
          echo "${VM_SSH_KEY}" | tr -d '\r' > ~/.ssh/ec2_key
          chmod 600 ~/.ssh/ec2_key
          ssh-keygen -y -f ~/.ssh/ec2_key > /dev/null || (echo "Invalid private key format provided in VM_SSH_KEY secret" && exit 1)

      - name: Deploy to VM
        env:
          VM_HOST: ${{ secrets.VM_HOST }}
          VM_USER: ${{ secrets.VM_USER }}
          ENV_FILE_CONTENTS: ${{ secrets.ENV_FILE_CONTENTS }}
          GHCR_TOKEN: ${{ secrets.GHCR_TOKEN }}
          IMAGE_NAME: "ghcr.io/${{ github.repository }}:latest"
          CONTAINER_NAME: "${{ github.event.repository.name }}_container"
        run: |
          echo "Connecting to ${VM_USER}@${VM_HOST}..."
          ssh -i ~/.ssh/ec2_key -o StrictHostKeyChecking=no -o ConnectTimeout=10 ${VM_USER}@${VM_HOST} /bin/bash <<EOF
            set -e

            echo "--> Setting up environment on VM..."
            CONFIG_DIR="\$HOME/.config/${CONTAINER_NAME}"
            SYSTEMD_USER_DIR="\$HOME/.config/systemd/user"
            ENV_FILE_PATH="\${CONFIG_DIR}/.env"

            mkdir -p "\${CONFIG_DIR}"
            mkdir -p "\${SYSTEMD_USER_DIR}"

            echo "${ENV_FILE_CONTENTS}" > "\${ENV_FILE_PATH}"

            echo "--> Verifying env file exists at: \${ENV_FILE_PATH}"
            ls -l "\${ENV_FILE_PATH}"
            echo "--- Env file contents: ---"
            cat "\${ENV_FILE_PATH}" || echo "Warning: Could not cat env file (might be empty)"
            echo "--------------------------"

            echo "--> Extracting PORT from \${ENV_FILE_PATH}..."
            VM_PORT=\$(grep '^PORT=' "\${ENV_FILE_PATH}" | cut -d '=' -f 2-)

            if [ -z "\${VM_PORT}" ]; then
              echo "ERROR: PORT variable not found or empty in \${ENV_FILE_PATH}"
              echo "Ensure your ENV_FILE_CONTENTS secret contains a line like 'PORT=8080'"
              exit 1
            fi
            echo "PORT extracted: \${VM_PORT}"

            echo "--> Logging into GHCR on VM..."
            echo "${GHCR_TOKEN}" | podman login ghcr.io -u ${{ github.actor }} --password-stdin

            echo "--> Pulling latest image: ${IMAGE_NAME}"
            podman pull ${IMAGE_NAME}

            SERVICE_NAME="${CONTAINER_NAME}.service"
            SYSTEMD_UNIT_PATH="\${SYSTEMD_USER_DIR}/\${SERVICE_NAME}"

            echo "--> Stopping and removing existing container (if any)..."
            if systemctl --user is-active --quiet "\${SERVICE_NAME}"; then
              echo "Stopping running systemd service: \${SERVICE_NAME}"
              systemctl --user stop "\${SERVICE_NAME}"
            else
              echo "Systemd service \${SERVICE_NAME} not active, attempting podman stop..."
              podman stop ${CONTAINER_NAME} || true
            fi

            podman rm ${CONTAINER_NAME} || true

            echo "--> Creating new container definition: ${CONTAINER_NAME}"
            podman create \
              -p "\${VM_PORT}:\${VM_PORT}" \
              --env-file "\${ENV_FILE_PATH}" \
              --name ${CONTAINER_NAME} \
              ${IMAGE_NAME}

            if [ \$? -ne 0 ]; then
                echo "ERROR: Failed to create container ${CONTAINER_NAME}"
                exit 1
            fi

            echo "--> Generating systemd unit file using redirection: \${SYSTEMD_UNIT_PATH}"
            podman generate systemd --new --name ${CONTAINER_NAME} > "\${SYSTEMD_UNIT_PATH}"

            if [ \$? -ne 0 ] || [ ! -f "\${SYSTEMD_UNIT_PATH}" ]; then
                echo "ERROR: Failed to generate systemd unit file at \${SYSTEMD_UNIT_PATH} using redirection."
                echo "Listing contents of \${SYSTEMD_USER_DIR}:"
                ls -la "\${SYSTEMD_USER_DIR}" || echo "Could not list directory."
                exit 1
            fi
            echo "Systemd unit file generated successfully."

            echo "--> Reloading systemd user daemon..."
            systemctl --user daemon-reload

            echo "--> Enabling and starting systemd service: \${SERVICE_NAME}"
            systemctl --user enable --now "\${SERVICE_NAME}"

            echo "--> Checking service status:"
            systemctl --user status "\${SERVICE_NAME}" --no-pager || echo "Warning: Service status check command failed, but deployment might still be okay."

            echo "--> Cleaning up GHCR login token..."
            podman logout ghcr.io || true

            echo "--> Deployment successful!"
          EOF

          SSH_EXIT_CODE=$?
          if [ ${SSH_EXIT_CODE} -ne 0 ]; then
            echo "Deployment script failed on remote host with exit code ${SSH_EXIT_CODE}."
            exit 1
          fi


